{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinson/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#adapted from  https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/\n",
    "\n",
    "#https://towardsdatascience.com/implementing-t-sne-in-tensorflow-manual-back-prop-in-tf-b08c21ee8e06\n",
    "#https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/\n",
    "#https://towardsdatascience.com/t-sne-python-example-1ded9953f26\n",
    "#https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNE \n",
    "\n",
    "\\begin{equation}\n",
    "q_{j|i}=\\frac{exp(-||y_i-y_j||^2)}{\\sum_{kdifi}exp(-||y_i-y_j||^2)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "C = \\sum_{i}KL(P_i||Q_i)=\\sum_i\\sum_jp_{j|i}log\\frac{p_{j|i}}{q_{j|i}}\n",
    "\\end{equation}\n",
    "\n",
    "Symmetric SNE\n",
    "\n",
    "\\begin{equation}\n",
    "q_{ij}=\\frac{exp(-||y_i-y_j||^2)}{\\sum_{kdifl}exp(-||y_k-y_l||^2)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_squared_euc_dists(X):\n",
    "    \"\"\"Compute matrix containing negative squared euclidean\n",
    "    distance for all pairs of points in input matrix X\n",
    "\n",
    "    # Arguments:\n",
    "        X: matrix of size NxD\n",
    "    # Returns:\n",
    "        NxN matrix D, with entry D_ij = negative squared\n",
    "        euclidean distance between rows X_i and X_j\n",
    "    \"\"\"\n",
    "    # Math? See https://stackoverflow.com/questions/37009647\n",
    "    sum_X = tf.reduce_sum(X**2, 1)\n",
    "    D = tf.transpose(-2 * tf.matmul(X, tf.transpose(X)) + sum_X) + sum_X\n",
    "    return -D\n",
    "\n",
    "def softmax(X, diag_zero=True):\n",
    "    \"\"\"Take softmax of each row of matrix X.\"\"\"\n",
    "\n",
    "    # We usually want diagonal probailities to be 0.\n",
    "    if diag_zero:\n",
    "        X = tf.matrix_set_diag(X,tf.zeros([X.shape[0].value],dtype=tf.float64))\n",
    " \n",
    "    return tf.nn.softmax(X)\n",
    "\n",
    "def calc_prob_matrix(distances, sigmas=None):\n",
    "    \"\"\"Convert a distances matrix to a matrix of probabilities.\"\"\"\n",
    "    if sigmas is not None:\n",
    "        two_sig_sq = 2. * tf.reshape(sigmas,(-1, 1))**2\n",
    "        return softmax(distances / two_sig_sq)\n",
    "    else:\n",
    "        return softmax(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symmetric SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "p_{ij} = \\frac{P + P^T}{2N}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "q_{ij}=\\frac{exp(-||y_i-y_j||^2)}{\\sum_{kdifl}exp(-||y_k-y_l||^2)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(eval_fn, target, tol=1e-10, max_iter=10000, \n",
    "                  lower=1e-20, upper=1000.):\n",
    "    \"\"\"Perform a binary search over input values to eval_fn.\n",
    "    \n",
    "    # Arguments\n",
    "        eval_fn: Function that we are optimising over.\n",
    "        target: Target value we want the function to output.\n",
    "        tol: Float, once our guess is this close to target, stop.\n",
    "        max_iter: Integer, maximum num. iterations to search for.\n",
    "        lower: Float, lower bound of search range.\n",
    "        upper: Float, upper bound of search range.\n",
    "    # Returns:\n",
    "        Float, best input value to function found during search.\n",
    "    \"\"\"\n",
    "    for i in range(max_iter):\n",
    "        guess = (lower + upper) / 2.\n",
    "        val = eval_fn(guess)\n",
    "        if val > target:\n",
    "            upper = guess\n",
    "        else:\n",
    "            lower = guess\n",
    "        if np.abs(val - target) <= tol:\n",
    "            break\n",
    "    return guess\n",
    "\n",
    "def calc_perplexity(prob_matrix):\n",
    "    \"\"\"Calculate the perplexity of each row \n",
    "    of a matrix of probabilities.\"\"\"\n",
    "    entropy = -np.sum(prob_matrix * np.log2(prob_matrix), 1)\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def perplexity(distances, sigmas):\n",
    "    \"\"\"Wrapper function for quick calculation of \n",
    "    perplexity over a distance matrix.\"\"\"\n",
    "    return calc_perplexity(numpy_calc_prob_matrix(distances, sigmas))\n",
    "\n",
    "\n",
    "def find_optimal_sigmas(distances, target_perplexity):\n",
    "    \"\"\"For each row of distances matrix, find sigma that results\n",
    "    in target perplexity for that role.\"\"\"\n",
    "    sigmas = [] \n",
    "    # For each row of the matrix (each point in our dataset)\n",
    "    for i in range(distances.shape[0]):\n",
    "        # Make fn that returns perplexity of this row given sigma\n",
    "        eval_fn = lambda sigma: \\\n",
    "            perplexity(distances[i:i+1, :], np.array(sigma))\n",
    "        # Binary search over sigmas to achieve target perplexity\n",
    "        correct_sigma = binary_search(eval_fn, target_perplexity)\n",
    "        # Append the resulting sigma to our output array\n",
    "        sigmas.append(correct_sigma)\n",
    "    return np.array(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_neg_squared_euc_dists(X):\n",
    "    \"\"\"Compute matrix containing negative squared euclidean\n",
    "    distance for all pairs of points in input matrix X\n",
    "\n",
    "    # Arguments:\n",
    "        X: matrix of size NxD\n",
    "    # Returns:\n",
    "        NxN matrix D, with entry D_ij = negative squared\n",
    "        euclidean distance between rows X_i and X_j\n",
    "    \"\"\"\n",
    "    # Math? See https://stackoverflow.com/questions/37009647\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    return -D\n",
    "\n",
    "def numpy_softmax(X, diag_zero=True):\n",
    "    \"\"\"Take softmax of each row of matrix X.\"\"\"\n",
    "\n",
    "    # Subtract max for numerical stability\n",
    "    e_x = np.exp(X - np.max(X, axis=1).reshape([-1, 1]))\n",
    "\n",
    "    # We usually want diagonal probailities to be 0.\n",
    "    if diag_zero:\n",
    "        np.fill_diagonal(e_x, 0.)\n",
    "\n",
    "    # Add a tiny constant for stability of log we take later\n",
    "    e_x = e_x + 1e-8  # numerical stability\n",
    "\n",
    "    return e_x / e_x.sum(axis=1).reshape([-1, 1])\n",
    "\n",
    "def numpy_calc_prob_matrix(distances, sigmas=None):\n",
    "    \"\"\"Convert a distances matrix to a matrix of probabilities.\"\"\"\n",
    "    if sigmas is not None:\n",
    "        two_sig_sq = 2. * np.square(sigmas.reshape((-1, 1)))\n",
    "        return numpy_softmax(distances / two_sig_sq)\n",
    "    else:\n",
    "        return numpy_softmax(distances)\n",
    "    \n",
    "def numpy_neg_squared_euc_dists(X):\n",
    "    \"\"\"Compute matrix containing negative squared euclidean\n",
    "    distance for all pairs of points in input matrix X\n",
    "\n",
    "    # Arguments:\n",
    "        X: matrix of size NxD\n",
    "    # Returns:\n",
    "        NxN matrix D, with entry D_ij = negative squared\n",
    "        euclidean distance between rows X_i and X_j\n",
    "    \"\"\"\n",
    "    # Math? See https://stackoverflow.com/questions/37009647\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    return -D\n",
    "\n",
    "\n",
    "def numpy_calc_prob_matrix(distances, sigmas=None):\n",
    "    \"\"\"Convert a distances matrix to a matrix of probabilities.\"\"\"\n",
    "    if sigmas is not None:\n",
    "        two_sig_sq = 2. * np.square(sigmas.reshape((-1, 1)))\n",
    "        return numpy_softmax(distances / two_sig_sq)\n",
    "    else:\n",
    "        return numpy_softmax(distances)\n",
    "    \n",
    "\n",
    "def numpy_p_conditional_to_joint(P):\n",
    "    \"\"\"Given conditional probabilities matrix P, return\n",
    "    approximation of joint distribution probabilities.\"\"\"\n",
    "    return (P + P.T) / (2. * P.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_joint(Y):\n",
    "    \"\"\"Given low-dimensional representations Y, compute\n",
    "    matrix of joint probabilities with entries q_ij.\"\"\"\n",
    "    # Get the distances from every point to every other\n",
    "    distances = neg_squared_euc_dists(Y)\n",
    "    # Take the elementwise exponent\n",
    "    exp_distances = tf.math.exp(distances)\n",
    "    # Fill diagonal with zeroes so q_ii = 0\n",
    "    exp_distances = tf.matrix_set_diag(exp_distances,tf.zeros([exp_distances.shape[0].value],dtype=tf.float64))\n",
    "    \n",
    "    # Divide by the sum of the entire exponentiated matrix\n",
    "    return exp_distances / tf.reduce_sum(exp_distances), None\n",
    "\n",
    "def p_conditional_to_joint(P):\n",
    "    \"\"\"Given conditional probabilities matrix P, return\n",
    "    approximation of joint distribution probabilities.\"\"\"\n",
    "    return (P + tf.transpose(P)) / (2. * P.shape[0].value)\n",
    "\n",
    "def p_joint(X, target_perplexity):\n",
    "    \"\"\"Given a data matrix X, gives joint probabilities matrix.\n",
    "\n",
    "    # Arguments\n",
    "        X: Input data matrix.\n",
    "    # Returns:\n",
    "        P: Matrix with entries p_ij = joint probabilities.\n",
    "    \"\"\"\n",
    "    # Get the negative euclidian distances matrix for our data\n",
    "    distances = numpy_neg_squared_euc_dists(X)\n",
    "    # Find optimal sigma for each row of this distances matrix\n",
    "    sigmas = find_optimal_sigmas(distances, target_perplexity)\n",
    "    # Calculate the probabilities based on these optimal sigmas\n",
    "    p_conditional = numpy_calc_prob_matrix(distances, sigmas)\n",
    "    # Go from conditional to joint probabilities matrix\n",
    "    P = numpy_p_conditional_to_joint(p_conditional)\n",
    "    return P\n",
    "\n",
    "\n",
    "\n",
    "def q_tsne(Y):\n",
    "    \"\"\"t-SNE: Given low-dimensional representations Y, compute\n",
    "    matrix of joint probabilities with entries q_ij.\"\"\"\n",
    "    distances = neg_squared_euc_dists(Y)\n",
    "    inv_distances = (1. - distances)**(-1)\n",
    "    inv_distances = tf.matrix_set_diag(inv_distances,tf.zeros([inv_distances.shape[0].value],dtype=tf.float64))\n",
    "    return inv_distances / tf.reduce_sum(inv_distances), inv_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_grad(P, Q, Y, inv_distances):\n",
    "    \"\"\"Estimate the gradient of t-SNE cost with respect to Y.\"\"\"\n",
    "    pq_diff = P - Q\n",
    "    pq_expanded = tf.expand_dims(pq_diff, 2)\n",
    "    y_diffs = tf.expand_dims(Y, 1) - tf.expand_dims(Y, 0)\n",
    "\n",
    "    # Expand our inv_distances matrix so can multiply by y_diffs\n",
    "    distances_expanded = tf.expand_dims(inv_distances, 2)\n",
    "\n",
    "    # Multiply this by inverse distances matrix\n",
    "    y_diffs_wt = y_diffs * distances_expanded\n",
    "\n",
    "    # Multiply then sum over j's\n",
    "    grad = 4. * tf.reduce_sum(pq_expanded * y_diffs_wt,1)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def backprop(C,P,Q,Y,Inv_distances):\n",
    "    grad = tf.gradients(C,Y)#grad_fn(P, Q, Y, inv_distances)#\n",
    "    update_Y = []\n",
    "    update_Y.append(tf.assign(m,m*beta1 + (1-beta1)*(grad)))\n",
    "    update_Y.append(tf.assign(v,v*beta1 + (1-beta1)*(grad**2)))\n",
    "    m_hat = m/(1-beta1)\n",
    "    v_hat = v/(1-beta2)\n",
    "    adam = lr/(tf.sqrt(v_hat)+adam_e)\n",
    "    update_Y.append(tf.assign(Y,Y-adam*m_hat))\n",
    "    return grad,update_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MACHINE_EPSILON = np.finfo(np.double).eps\n",
    "MACHINE_EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'destilation_tower': 0, 'dynamic_equipment': 1, 'container': 2, 'pressure_vessel': 3, 'valves': 4, 'handrail': 5, 'gratting_floor': 6, 'structural_component': 7, 'stairs': 8, 'heat_exchanger': 9}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = 'dataset/train'\n",
    "classes = os.listdir(root)\n",
    "class_dict = {k:v for v,k in enumerate(classes)}\n",
    "print(class_dict)\n",
    "filenames = []\n",
    "labels = []\n",
    "for i in classes:\n",
    "    path = os.path.join(root,i)\n",
    "    files = os.listdir(path)\n",
    "    for ii in files:\n",
    "        filenames.append(os.path.join(path,ii))\n",
    "        labels.append(class_dict[i])\n",
    "        #print(img_path,i,class_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "\n",
    "    # Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    # This will convert to float values in [0, 1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    image = tf.image.resize_images(image, [224, 224])\n",
    "    \n",
    "    return tf.squeeze(tf.reshape(image,[-1,224*224*3])), label\n",
    "\n",
    "def train_preprocess(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "\n",
    "    # Make sure the image is still in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "batch_size = len(filenames)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np.array(filenames),\\\n",
    "                                              np.array(labels)))\n",
    "dataset = dataset.shuffle(len(filenames))\n",
    "dataset = dataset.map(parse_function, num_parallel_calls=4)\n",
    "#dataset = dataset.map(train_preprocess, num_parallel_calls=4)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4232"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = int(len(filenames)/1000)+1\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/robinson/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(4232, 150528)\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer\n",
    "\n",
    "# Actually run in a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(1):\n",
    "        X,y = sess.run(next_element)\n",
    "        print(X.shape)\n",
    "N = X.shape[0]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4232"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\"\"\"\n",
    "root ='tsne/images'\n",
    "images = os.listdir(root)\n",
    "X = []\n",
    "for i in images:\n",
    "    img = os.path.join(root,i)\n",
    "    img = Image.open(img).resize((200,200))\n",
    "    img = np.array(img).flatten()\n",
    "    X.append(img)\n",
    "X = np.vstack(X)\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_digits\n",
    "#X, y = load_digits(return_X_y=True)\n",
    "\n",
    "\n",
    "q_fn = q_tsne\n",
    "grad_fn = tsne_grad\n",
    "N,D = X.shape\n",
    "d = 2\n",
    "PERPLEXITY = 30\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "adam_e = MACHINE_EPSILON\n",
    "lr = 1e-1\n",
    "grad_fn = tsne_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = tf.placeholder(tf.float64,shape=[N,N])\n",
    "\n",
    "Y = tf.get_variable(\"Y\",shape=[N,d],dtype=tf.float64,initializer=tf.random_normal_initializer())\n",
    "\n",
    "m,v = tf.Variable(tf.zeros_like(Y)),tf.Variable(tf.zeros_like(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, inv_distances = q_fn(Y)\n",
    "\n",
    "Q= tf.maximum(Q,MACHINE_EPSILON)\n",
    "\n",
    "C = tf.reduce_sum( tf.reshape(P,[-1]) * tf.log(tf.maximum(tf.reshape(P,[-1]),MACHINE_EPSILON)\\\n",
    "                                               / tf.reshape(Q,[-1])))#(tf.reshape(Q,[-1])+tf.constant(1.e-10,dtype=tf.float64))))\n",
    "\n",
    "grad = grad_fn(P, Q, Y, inv_distances)\n",
    "update_Y = []\n",
    "update_Y.append(tf.assign(m,m*beta1 + (1-beta1)*(grad)))\n",
    "update_Y.append(tf.assign(v,v*beta1 + (1-beta1)*(grad**2)))\n",
    "m_hat = m/(1-beta1)\n",
    "v_hat = v/(1-beta2)\n",
    "adam = lr/(tf.sqrt(v_hat)+adam_e)\n",
    "update_Y.append(tf.assign(Y,Y-adam*m_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 -- KL_Loss 2.049234\n",
      "iteration 10 -- KL_Loss 2.046416\n",
      "iteration 20 -- KL_Loss 2.041901\n",
      "iteration 30 -- KL_Loss 2.037013\n",
      "iteration 40 -- KL_Loss 2.032037\n",
      "iteration 50 -- KL_Loss 2.027061\n",
      "iteration 60 -- KL_Loss 2.022116\n",
      "iteration 70 -- KL_Loss 2.017209\n",
      "iteration 80 -- KL_Loss 2.012343\n",
      "iteration 90 -- KL_Loss 2.007517\n",
      "iteration 100 -- KL_Loss 2.002731\n",
      "iteration 110 -- KL_Loss 1.997984\n",
      "iteration 120 -- KL_Loss 1.993275\n",
      "iteration 130 -- KL_Loss 1.988604\n",
      "iteration 140 -- KL_Loss 1.983969\n",
      "iteration 150 -- KL_Loss 1.979371\n",
      "iteration 160 -- KL_Loss 1.974807\n",
      "iteration 170 -- KL_Loss 1.970277\n",
      "iteration 180 -- KL_Loss 1.965780\n",
      "iteration 190 -- KL_Loss 1.961317\n",
      "iteration 200 -- KL_Loss 1.956885\n",
      "iteration 210 -- KL_Loss 1.952484\n",
      "iteration 220 -- KL_Loss 1.948115\n",
      "iteration 230 -- KL_Loss 1.943776\n",
      "iteration 240 -- KL_Loss 1.939466\n",
      "iteration 250 -- KL_Loss 1.935186\n",
      "iteration 260 -- KL_Loss 1.930934\n",
      "iteration 270 -- KL_Loss 1.926710\n",
      "iteration 280 -- KL_Loss 1.922514\n",
      "iteration 290 -- KL_Loss 1.918345\n",
      "iteration 300 -- KL_Loss 1.914203\n",
      "iteration 310 -- KL_Loss 1.910087\n",
      "iteration 320 -- KL_Loss 1.905997\n",
      "iteration 330 -- KL_Loss 1.901931\n",
      "iteration 340 -- KL_Loss 1.897888\n",
      "iteration 350 -- KL_Loss 1.893867\n",
      "iteration 360 -- KL_Loss 1.889868\n",
      "iteration 370 -- KL_Loss 1.885888\n",
      "iteration 380 -- KL_Loss 1.881927\n",
      "iteration 390 -- KL_Loss 1.877983\n",
      "iteration 400 -- KL_Loss 1.874056\n",
      "iteration 410 -- KL_Loss 1.870145\n",
      "iteration 420 -- KL_Loss 1.866249\n",
      "iteration 430 -- KL_Loss 1.862368\n",
      "iteration 440 -- KL_Loss 1.858501\n",
      "iteration 450 -- KL_Loss 1.854647\n",
      "iteration 460 -- KL_Loss 1.850805\n",
      "iteration 470 -- KL_Loss 1.846973\n",
      "iteration 480 -- KL_Loss 1.843149\n",
      "iteration 490 -- KL_Loss 1.839333\n",
      "iteration 500 -- KL_Loss 1.835522\n",
      "iteration 510 -- KL_Loss 1.831715\n",
      "iteration 520 -- KL_Loss 1.827911\n",
      "iteration 530 -- KL_Loss 1.824109\n",
      "iteration 540 -- KL_Loss 1.820310\n",
      "iteration 550 -- KL_Loss 1.816512\n",
      "iteration 560 -- KL_Loss 1.812713\n",
      "iteration 570 -- KL_Loss 1.808914\n",
      "iteration 580 -- KL_Loss 1.805114\n",
      "iteration 590 -- KL_Loss 1.801312\n",
      "iteration 600 -- KL_Loss 1.797507\n",
      "iteration 610 -- KL_Loss 1.793700\n",
      "iteration 620 -- KL_Loss 1.789890\n",
      "iteration 630 -- KL_Loss 1.786077\n",
      "iteration 640 -- KL_Loss 1.782261\n",
      "iteration 650 -- KL_Loss 1.778442\n",
      "iteration 660 -- KL_Loss 1.774620\n",
      "iteration 670 -- KL_Loss 1.770795\n",
      "iteration 680 -- KL_Loss 1.766966\n",
      "iteration 690 -- KL_Loss 1.763135\n",
      "iteration 700 -- KL_Loss 1.759300\n",
      "iteration 710 -- KL_Loss 1.755462\n",
      "iteration 720 -- KL_Loss 1.751622\n",
      "iteration 730 -- KL_Loss 1.747780\n",
      "iteration 740 -- KL_Loss 1.743936\n",
      "iteration 750 -- KL_Loss 1.740091\n",
      "iteration 760 -- KL_Loss 1.736244\n",
      "iteration 770 -- KL_Loss 1.732394\n",
      "iteration 780 -- KL_Loss 1.728542\n",
      "iteration 790 -- KL_Loss 1.724685\n",
      "iteration 800 -- KL_Loss 1.720825\n",
      "iteration 810 -- KL_Loss 1.716962\n",
      "iteration 820 -- KL_Loss 1.713097\n",
      "iteration 830 -- KL_Loss 1.709230\n",
      "iteration 840 -- KL_Loss 1.705362\n",
      "iteration 850 -- KL_Loss 1.701492\n",
      "iteration 860 -- KL_Loss 1.697621\n",
      "iteration 870 -- KL_Loss 1.693750\n",
      "iteration 880 -- KL_Loss 1.689879\n"
     ]
    }
   ],
   "source": [
    "#X = np.random.rand(N,D)\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "\n",
    "def update(i):\n",
    "    label = 'timestep {0}'.format(i)\n",
    "    print(label)\n",
    "    sc.set_offsets(np.c_[b[-1][:,0]\\\n",
    "                                 ,b[-1][:,1]])\n",
    "    return sc\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    images = []\n",
    "    sess.run(init_op)\n",
    "    X,y = sess.run(next_element)\n",
    "    pp=p_joint(X,PERPLEXITY)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for i in range(10000):\n",
    "        sess.run(init_op)\n",
    "        #for i in range(NN):\n",
    "        X,y = sess.run(next_element)\n",
    "        b,c,qq = sess.run([update_Y,C,Q],feed_dict={P:pp})\n",
    "        xx,yy=b[-1].T\n",
    "          \n",
    "        if i%10==0: \n",
    "            print(\"iteration %d -- KL_Loss %f\"%(i,c))\n",
    "            plt.title('KL_loss: {}'.format(c))\n",
    "            img = [plt.scatter(b[-1][:,0],\\\n",
    "                            b[-1][:,1],\\\n",
    "                            c=y)]\n",
    "            \n",
    "            images.append(img)\n",
    "            \n",
    "    ani = ArtistAnimation(fig, images,interval=500)\n",
    "    ani.save(\"class.mp4\")\n",
    "    plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
